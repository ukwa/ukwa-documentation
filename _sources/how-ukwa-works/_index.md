# Goals & Principles

The UK Web Archive (UKWA) collects millions of websites each year, and have been collecting websites since 2005. We do this for posterity, but also for readers and researchers today.

We enable curators and collaborators to define what we should collect, and how often. We attempt to visit every UK website at least once a year, and for the sites the curators have identified we can visit much more frequently. For example, we collect news sites at least once per day.

We capture the websites using web crawling software, and converting the live sites into static records we can preserve for the future.

We use these records to reconstruct an imperfect facsimile of the original live site, allowing our readers and researchers to browse the UK web as it was in the past. We can also analyse these historical records as data, to extract historical trends, or to build access tools.

## Use Cases

For our Readers, we want them to be able to:

- **View** archive web pages as the were in the past, if they know what URL they are interested in.
- **Browse** curated collections of archived web pages and sites.
- **Search** for web pages to find what they need.
- **Analyse** our archived web pages, exploring the collection as a dataset.

To meet these needs, we need to populate the archive. To do this, our _Archivists_ and _Curators_ need to:

- **Collect** live web pages, by directing our web crawlers to capture them on various frequencies.
- **View** what the archived web pages the crawler collected, to check how well it's working.
- **Describe** the archived web pages and sites, to help document today's web and make it easier for _Readers_ to find.
